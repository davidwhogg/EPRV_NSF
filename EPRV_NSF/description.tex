% This file is part of the SolveAllOfAstronomy project.
% The GetRichOrDieTrying project is a different thing entirely.
% Copyright 2019 David W. Hogg and Megan Bedell

% # Style notes
% - Third person mainly (but maybe not exclusively -- use first for emphasis!)
% - Use macros for acronyms, project names, and so on!

% # To-do list
% - Do we need to explain wobble in greater detail?
% - Choose more or different sentences or phrases to \textbf{}

\documentclass[12pt, letterpaper]{article}
\usepackage{fancyhdr, varioref}
\setlength{\headheight}{3ex}
\setlength{\headsep}{2ex}
\input{hogg_nsf}
  \renewcommand{\headrulewidth}{0pt}
  \pagestyle{fancy}
  \lhead{\textcolor{darkgrey}{\textsf{\acronym{CDS\&E}: Precision measurement of stellar \RV s in the presence of intrinsic variability}}}
  \rhead{\textcolor{darkgrey}{\textsf{\thepage}}}
  \cfoot{}
\begin{document}\sloppy\sloppypar\raggedbottom\frenchspacing

\noindent
The detection, confirmation, and detailed characterization of exoplanets
requires extreme precision radial-velocity (\EPRV)
measurements.
With \EPRV\ data we can measure the 
masses (and compositions) of transiting planets,
characterize host stars in exquisite detail,
and make measurements of stellar and planetary surfaces.
%Indeed, \EPRV\ was awarded the 2019 Nobel Prize in Physics! -- what does that have to do with anything?
Most importantly for this proposal, with the next generation of \EPRV\ 
instruments we hope to detect terrestrial planets on Earth-like orbits.

From a fundamental-physics perspective, \emph{there is essentially no limit to how
precisely astronomers might measure the radial velocity variations
of the center of mass of a distant star with a spectrograph}.
Of course in real life, there are real limits, set
by (for example) photon noise, stellar surface convection and activity,
variations in the Earth's atmosphere, and spectrograph calibration stability.
In practice, almost no stars \emph{reported in the literature}
have had their center-of-mass
velocities measured consistently with an empirical scatter
smaller than $\approx 100\,\cmps$ 
($1\,\mps$) (although there are rumors of better from \ESPRESSO\ \citep{Pepe2010} and \EXPRES\ \citep{Jurgenson2016}).
Because most \EPRV\ discoveries and characterizations have been made
at or near precision limits---that is, near what is detectable at noise levels
greater than $1\,\mps$ per epoch---\textbf{any improvement in precision directly
translates into increased scientific return on investment},
both from existing archival data and from new data from new programs.

\paragraph{Stellar variability is the ``tall pole'':}
Right now the best spectrographs are obtaining internal calibration
consistencies of a few $\cmps$ \citep{espresso-eprv4, expres-eprv4}.
These internal precisions are determined by repeatability and
stability of calibration references such as arc lamps, etalon
(Fabry--Perot) interferometer signals, gas cells, or laser-frequency
combs.

It is therefore now accepted (and convincingly
established) that the majority of the radial-velocity variance
in \EPRV\ campaigns is coming from the stars themselves, or what's
often called ``astrophysical noise'' (convection, star spots,
asteroseismic pulsations, plages, faculae, and flares, for example).
This proposal is, therefore, \textbf{to address the dominant sources of
  noise in \EPRV\ experiments and to create working methods to
  mitigate them}.
You can precisely calibrate the variations in the spectrograph.
Here we ask: Can you also precisely calibrate the variations in the star?
We believe that the answer is \emph{yes}.

%There is also noise coming from telluric lines in the Earth's atmosphere
%(especially unmodeled tiny micro-tellurics).
%In a separate but related project \citep{Bedell2019} we are addressing this
%source of noise as well.
%Our work suggests that tellurics and micro-tellurics are addressable problems
%and do not currently dominate the end-to-end noise budget.
%But it is certainly also an important area for work and study.

\textbf{Our approach to stellar variability is to build data-driven models of what
we don't understand, but with model structure carefully constructed to
match what little we \emph{do} understand.}
That is, to build flexible models which have their capacity tuned to
capture the variations we expect to see. 
Such models will capitalize on our physics-based expectations to
maximize interpretability, while simultaneously having the flexibility to absorb 
the ``unknown unknown'' effects of stellar variability.
This approach is closely linked to previous work we have done in the related 
area of \textit{telluric mitigation} for \EPRV\ \citep{Bedell2019}.
%The proposers have strong cred in this area.

\paragraph{Why this team?}
The proposing team has a demonstrated record of expertise in making
measurements of stars at the limits of observational precision. 
\PI\ Hogg has built novel data-driven models of the color--magnitude diagram of
stars, improving ESA \Gaia\ parallax measurements (\citealt{Leistedt, Widmark, Anderson}).
These models re-purpose machine-learning tools to build models that
have causal structure useful to astronomers (they know about
luminosity distances and parallaxes, for example).
Co-I Bedell has developed new techniques to make exceedingly
precise measurements of stellar element abundances with minimal
reliance on stellar models, by designing perfectly differential stellar
comparisons \citep{Bedell2014, Bedell2018}.
\PI\ Hogg has the current best-in-class methods for determining stellar
parameters and abundances from survey-quality spectroscopy (\citealt{Cannon2, Doppel}).
These methods (called \project{The Cannon} after Annie Jump) are
data-driven but designed to capture the smooth variations (with
respect to parameters) that we expect in stellar spectra.

And most relevant of all, \textbf{\PI\ Hogg and Co-I Bedell built an open-source
pipeline for \EPRV\ data analysis} (\wobble; \citealt{Bedell2019}).
This pipeline has many novel aspects to it, but some of the key ideas
are the following: 
It separates stellar and telluric signals in the data in a purely
data-driven way.
It delivers Fisher-information-saturating \RV\ measurements without any
theory-derived or external stellar template; it gets the stellar model
almost losslessly from the data themselves.
It makes use of \project{TensorFlow} and ideas from convex optimization
for computational tractability and performance.
It permits telluric variability and can be extended to permit stellar
variability (that's this proposal).
And it is extensible, flexible, and open source, so that it can be
tweaked and used by different teams with different hardware and goals.

In addition to building this software, the proposing team is known for
convening and running hack-week-style workshops (such as \project{Gaia
  Sprint}, \project{Preparing for TESS}, and \project{Telluric Line
  Hack Week}) in which participants learn to use new data sets, new
data-analysis methods, and new software for their scientific goals.
Since a big part of this proposal is dissemination and community adoption,
this track record is very relevant to the current proposal.
And for the long-term value and sustainability of the open-source methods
and software we develop.

Finally, this team has close ties to multiple \EPRV\ instrument and survey
teams, including the \project{Terra Hunting Experiment} and the \project{EXPRES}
spectrograph. Together with Co-I Bedell's extensive experience with utilizing 
publically-available \project{HARPS} data, these connections ensure that 
we can obtain the data sets we need, and that the findings we produce will 
be noticed by the \EPRV\ community.

\paragraph{Relevance to \NSF\ programs and objectives:}

We established relevance of this general set of problems to the \acronym{AAG} program
in an email conversation with program officer Neff, although given the fundamental
astronomical and physics issues at stake, it seems obvious that it is related to
\acronym{AAG}.
We have also marked this proposal with the \acronym{CDS\&E} tag.
This proposal falls into the \acronym{CDS\&E} category because it is not just a data
analysis proposal; it is about changing the way we do data analysis, to capitalize on
and scale with enormous increases in data volume and precision.
We are moving the \EPRV\ community away from fixed templates to data-driven models,
which capture the details of each star more accurately.
We are reducing the assumptions underlying stellar variability approaches, and using
the overwhelming amounts of data to make the decisions.
We are making use of \project{TensorFlow} and other machine-learning tools to ensure
scalability as the data grow larger.

In addition to the \acronym{AAG} and \acronym{CDS\&E} relevance, this proposal touches on
various of the \NSF\ 10 Big Ideas from a couple of years ago.
For example, the use of machine-learning infrastructure and applied-math ideas to model
stellar spectra is very aligned with the \textbf{Growing Convergence Research} theme.
Astronomers can't solve these problems on our own!
And the fact that we are building software components that make new spectrographs more
precise, more productive, and more efficient means that we connect strongly to the
\textbf{Mid-scale Research Infrastructure} theme.
We want these projects to be the best that they can be.
In principle we also connect to the \textbf{Understanding the Rules of Life} theme! 
Although admittedly we are still years away from finding
Earth 2.0, our proposed work will be a stepping stone on that path.

\hoggbox{\paragraph{This proposal, in a nutshell:}
Stellar variability is the tall pole in \EPRV\ at the present day.
We propose to build customized, data-driven models to capture and mitigate these
stellar astrophysical noise sources, taking advantage of modern advances in
data science and applied math.
Our driving motivation is to make \EPRV\ measurements an order of magnitude more precise
and thereby transform the science
that can be done with the current and next generation of spectrographs. 
Along the way, our contributions to the literature will 
establish a common statistical framework to enable communication 
and methodological comparisons across the global \EPRV\ community. 
}

\section{First Stage: Bounds on \EPRV\ in time-varying stars (information)}

Perhaps the strangest thing about our science goal is that
it is \emph{conceptually impossible}!

Okay, not really, but it is worth remembering that the entire
\EPRV\ program is built on the premise that you can measure
\emph{relative} velocities far more precisely than you can measure
\emph{absolute} velocities, because the relative velocity between two
exposures of the same star is seen as a spectral shift from one
exposure to the next.
This point depends on the assumption that the spectrum does not vary
between exposures!
Or that it doesn't vary \emph{adversarially} between exposures.

\hoggbox{\paragraph{Information Theory:}
The fundamental limit on \EPRV\ precision is given by the Cram\'er--Rao Bound, or
the Fisher Information.
The Fisher Information is the best possible inverse-variance that can be obtained
on any \RV\ measurement, and it appears in the literature,
although not usually written in this language \citep{Butler1996, Bouchy2003}.
Symbolically, in the time-invariant case,
the Information $\sigma_v^{-2}$ is given by
\begin{equation}\label{eq:fisher}
\sigma_v^{-2} =
\left[\frac{\dd f}{\dd v}\right]^{\mathsf T}\cdot C^{-1}\cdot
\left[\frac{\dd f}{\dd v}\right] \quad ,
\end{equation}
where $\sigma_v$ is the \RV\ uncertainty, the derivatives are
of the spectral expectation $f$ (seen as a column vector) with respect to \RV\ $v$,
and the inverse variance of the noise in the spectrum is $C^{-1}$ (diagonal in the
simple case that all spectral pixels are independently measured).

Things get a bit more complicated when there are nuisance parameters, but
not fundamentally different: The derivatives
become rectangular projection operators and the Information becomes a matrix
that must be inverted to deliver nuisance-marginalized uncertainties.

When we turn on stellar variability, there will be two ways to modify the Fisher
Information.
In one framework, the stellar variability is seen as an additional noise
in the space of wavelength and time.
The Information gets modified by having $f$ become not just one spectrum but a
full time series of spectra, and by having $C$ become not just the diagonal tensor
of the instrumental noise, but also of the non-diagonal ``noise'' associated with the spectral
variability (which will be covariant in both wavelength and time).
The additional noise will increase the eigenvalues of $C$, decrease the eigenvalues
of $C^{-1}$, and decrease precision (increase $\sigma_v$).

In another framework, stellar variability is seen as an additional set of
nuisance parameters (for example, the amplitudes of eigenspectral contributions
at each observation).
In this framework, the stellar variability
adds dimensions to the Information tensor.
This will also serve to decrease precision (increase $\sigma_v$).
}

Once the spectrum is permitted to vary between exposures (and we know that
it does), the fundamental assumptions of the \EPRV\ literature break down.
In the First Stage (information) we propose to fix this problem, by
working out the conceptual framework for thinking about
\textbf{information theory in \EPRV\ in the presence of a variable
  spectrum}.
We have identified a few approaches to this problem, and we intend to
work down all paths.
This First Stage will produce papers from each of the paths that proves
to deliver useful results or frameworks.
This is the \emph{theory part} of this proposal.

The question is: How precisely can a stellar RV be measured in the context
of stellar spectral variability?
Of course the answer depends on exactly how the star is varying;
in detail it depends on questions about the variability like the following:
\begin{itemize}
\item
\emph{What is the amplitude of the variability?}
If the amplitude is tiny, it won't strongly affect \RV\ measurements.
\item
\emph{What is the dimensionality of the spectral variability?}
If the spectrum varies in a low-dimensional space spanned by a small set
of eigen-spectra, \EPRV\ measurements can be made in the tangent space
orthogonal to the spectral variability.
\item
\emph{What is the time coherence of the variability?}
If the variability is uncorrelated from observation to observation, it
can be mitigated by repeat exposures. If it isn't, more complicated
strategies are required.
\item
\emph{What is the projection of the spectral variability onto the RV derivative?}
The most adversarial kind of spectral variability is that which looks
like the derivative of the spectrum with respect to \RV.
\end{itemize}

\noindent
All of these issues will come up again below in the Second Stage (p-modes)
and Third Stage (stochastics).
But in the First Stage, we want to understand precisely and quantitatively
how these issues (amplitude, dimensionality, time coherence, and projection
onto RV derivative) affect fundamental limits on the precision of
\EPRV\ measurements.
\textbf{That is, in the First Stage we are trying to understand what is theoretically
possible for \EPRV.}

There are four approaches we will take in the First Stage (information)
to answering these theory questions,
and the exact focus of the papers we write in the First Stage 
depend on our success with each
approach, and how appropriate each approach is, given the magnitudes, spectral
shapes, and time structures of the noise sources we find in the data (and in
the Second and Third Stages). 
We emphasize that even ``failure'' of an approach is still a scientifically 
valuable finding, because it gives us new insight into the qualities 
of \EPRV\ noise. 
Our four overarching approaches are the following:
\begin{itemize}
\item
\textbf{Pure noise model:}
If the stellar spectral variability has low amplitude, and either has
no spectral or time structure, or else we are pessimistic about
modeling it in any way, we face the \emph{worst-case scenario}.
In this scenario, all we can do is add the spectral noise to the photon noise
in the noise tensor $C$ in the Fisher Information definition (\ref{eq:fisher}),
in the box \vpageref{eq:fisher},
maintaining the spectral correlations in the noise (which are important),
and re-compute the Information in this bad context.
The Fisher Information will get smaller as the stellar variability
gets worse, and the resulting theoretical \EPRV\ uncertainties will
get larger.

In this approach, the idea would be to estimate (either empirically or
theoretically) the excess variance (in spectrum space) induced by the stellar
spectral variability.
This estimation could be performed with something like PCA, or a more
sophisticated latent-variable model (\eg, that of \citealt{HMF}).
That excess variance would be noise that is covariant in wavelength space,
but which could be added directly to the instrument (photon and calibration
and tellurics) noise tensor $C$ in the Information tensor (\ref{eq:fisher}).
The goal of this approach would be to demonstrate how an investigator
can estimate quantitatively the impact of various kinds of observed or
theorized stellar variability, if the investigator has an estimate of
its variance tensor in the observed space.
\item
\textbf{Spectral filtering:}
If the variability in the spectral domain is low-dimensional, then it is
possible to project the data into a space that is orthogonal to the
variability subspace.
This projection will cost some information, but it will return a time-invariant
spectrum and return us to the utopia of the time-invariant Fisher Information
calculation, just with a new noise model.

In this approach, the idea would be to estimate the (hopefully
low-rank) subspace (eigenspace) in which the stellar spectrum varies.
Once again, this could be estimated with PCA or more sophisticated
methods.
Then the idea is that at each epoch, the spectrum of the star is
described by a mean spectrum, a \RV, and a set of amplitudes with
which the eigenspectra co-add to fit the individual spectrum taken at
that epoch.
These amplitudes become nuisance parameters which complexify the
Information computation, as described in the box \vpageref{eq:fisher}.
This method is called ``filtering'' because this information-theoretic
operation is equivalent to projecting the data onto a subspace that is
tangent to the spectral-variability subspace.
That will look like a linear filter of the data. 
The effectiveness of this method will depend on how closely the variability
subspace is aligned with the derivative of the spectrum with respect to the \RV. 
\item
\textbf{Time-domain process priors:}
In the limit of long data sets (years), no source of stellar spectral
variability will look identical to a ``Kepler process'', or the time
dependence of \RV\ induced by mixtures of two-body orbits.
Thus, even in the presence of pretty adversarial stellar spectral
variability, \EPRV\ precision can be reobtained by observing for long
times, and separating the final \RV\ measurements over time into those
components that can and cannot be generated by a Kepler process.
These approches work well in certain limits, and work best when we can
understand the time correlations of the spectral variability quantitatively,
or when those time correlations are on time scales far from the
orbital times of interest.

In this approach, the idea would be to put a parameterized, rigid
prior on the exoplanet-induced signals (that is, that they are produced
by an N-body Keplerian system), and a stochastic prior (like a Gaussian
Process) on the stellar-variability-induced signals, treating the
\RV\ measurements as being created by the sum of these two
processes.
The information-theoretic question would then be asked in the space
of the Kepler parameters instead of the \RV\ measurements themselves.
And the Gaussian Process would become part of the nuisance space of
the problem.
This would require a substantial reformulation of the information
theory written in the box \vpageref{eq:fisher}, but it is not
difficult in principle.
It would end up answering a different question, however:
We would not ask ``how well can we measure \RV?''; we would ask
``how well can we learn Kepler parameters?'' (work along these lines
is underway also in the E.~Ford group at PSU, private communication).
This is the best approach if the stellar variability \emph{does} look
like or contain within its subspace the derivative of the mean spectrum
with respect to \RV.
That is, this is the approach for worst-case scenarios.
In non-worst-case it is still useful, however, because this approach
can be combined with the other approaches in this First Stage (information) to deliver
information-theoretic limits on Kepler parameters in the face of
different kinds of spectral variability.
\item
\textbf{Causal modeling:}
Finally, for long-term stellar monitoring campaigns, there is an idea
from causal inference that can help us:
If the stellar intrinsic variability is uncorrelated with the orbits
of any planets (and we expect this except for extremely short-period,
locked planets), then there is no sense in which the spectral
variations should be able to \emph{predict} the \RV\ variations that
are planet-induced.
That is, a regression might be able to separate out the intrinsic
stellar variability.
This approach will only work reliably in projects performing very
long-term stellar monitoring, because it is justified only in a hard
limit.
However, there are many such projects starting now.
(This method is conceptually related to things we have done with
\Kepler\ and \Ktwo\ for planet discovery; \citealt{DFMK2, CPM, CPMdiff}.)

This approach is similar to the previous approach (time-domain processes),
but we determine the impact of the variability on the \RV\ measurements
by considering the extent to which the stellar spectrum variations
associated with the induced \RV\ measurements can be used to predict
the \RV.
The idea is to perform regressions and look at the predictive power, and
turn statistics of that predictive power into a quantitative estimate
of the impact on \EPRV\ precision.
The impact will be related to the covariance of the spectral variation
and the \RV\ variation; that is, it will be possible to estimate it
empirically.
\end{itemize}

\noindent
As we work down these approaches, we will develop results and assemble them
into something like two papers on the theory of \EPRV\ in the presence of
stellar spectral variability. \textbf{These two papers, and the associated open-source
code we generate to perform the justifying experiments, will constitute the
key deliverables of this First Stage (information).}
However, this Stage will also deliver advice and methods to
\EPRV\ investigators.
These methods will permit investigators to determine the impact of
various kinds of empirically observed stellar variability on their
end-to-end expected precision,
and help them to understand whether their end-to-end results are
achieving the \RV\ precisions they ought to expect.
\textbf{That is, we will deliver methods for estimating variability, and methods
for estimating the quantitative impact of those variabilities on \RV\ precision.}

One important point to make in the papers we write is the following:
In the case that the stellar spectrum does not vary, the
cross-correlation function (\CCF) between the observed spectrum and a
(very good) spectral template (model spectrum) is a \emph{sufficient
  statistic} for the \RV\ measurement.
That is, the \CCF\ contains all of the \RV\ information in the data.
This is \emph{no longer true} once the spectrum starts to vary!
That means that approaches for dealing with stellar spectral variability
that are confined to considerations about the shape of the \CCF\ are
doomed to fail---or at least doomed to under-perform.
\textbf{Once the spectrum starts to vary, we have to adopt approaches that model
the data in the full two-dimensional space of photon wavelength and time.}
One of our primary high-level goals is to move the community in the
right direction on this point.

\section{Second Stage: Mitigation of asteroseismic variability (p-modes)}

Asteroseismic p-modes deliver a significant source of contamination in \EPRV\
measurements. 
As the star pulsates, its surface sinusoidally moves towards and away from
the observer, imparting Doppler shifts at the $\mps$-level or worse.
Traditionally these oscillations are dealt with by ``integrating over'' them
with a 15-minute exposure, but recent works have cast doubt on the efficiency 
of this approach \citep{Medina2018, Chaplin2019}. 
While the best methods for dealing with p-modes in \EPRV\ have yet 
to be convincingly established (and we intend to make progress here),
we can regard this noise source as a kind of
\textit{best-case} scenario for correlated noise. That is, we believe that 
p-modes can be nearly completely mitigated through spectral and temporal modeling, 
because they have two important properties:
The first is that
\textbf{p-modes are temporally coherent} on reasonable time scales (days to months, depending on
stellar type). Therefore, they can be fit with a stationary model over time intervals 
short relative to the coherence time. 
The second important property is that
\textbf{p-modes have associated temperature signatures}. Indeed, the star gets
hotter when it contracts and cooler when it expands, adiabatically. Although for
G-type stars these temperature variations are tiny, they ought to be visible in
high-signal-to-noise \EPRV\ spectral observations.

These nice properties of p-modes, and the relatively straightforward physical 
interpretation of these oscillations, make them an ideal laboratory for testing
various stellar noise mitigation strategies. \textbf{The goal of the Second Stage is 
two-fold: we want to produce recommendations for optimal treatment of p-modes
in \EPRV\ measurements, but we also want to use p-modes as an 
example of how to work through correlated stellar noise from within the 
frameworks we established in the First Stage publications.}

We plan to explore four mitigation strategies for p-modes. We will stage 
illustrative experiments using a toy model of a Sun-like oscillator. 
We will also follow up using real data, mainly from short-exposure 
\EXPRES\ observations of $\tau$ Ceti. Both of these data sets are already
in-hand. The mitigation strategies we will explore are:
\begin{itemize}
\item
\textbf{Choosing resonant integration times:}
There is hope in the \EPRV\ community that cleverly chosen integration
times can obviate p-mode noise \citep[see, \eg][]{Chaplin2019}.
We will test the performance of this approach using
both our realistic toy model of stochastic p-mode oscillations
and binned versions of the real \EXPRES\ data. 
These results will serve as a benchmark of the best-case outcome 
using a passive avoidance strategy.
\item
\textbf{Resolving modes and fitting them coherently:}
The alternative strategy to p-mode avoidance is to take short exposures,
short enough that they Nyquist-sample the highest-frequency p-modes of
the star.
Splitting a long exposure into many short exposures 
incurs some observing and read-noise overheads, but permits
resolution of the p-modes directly.
Then the obtained \RV\ measurements can be fit with a sum of coherent
sinusoids, regularized with the power spectrum of the
star. 
Said power spectrum might be known (from \TESS or comparable photometry) or,
in some cases, it might be learned from the \RV\ data itself. 
Either way, this method promises to correct the data within an exposure set 
self-consistently with uncertainties correctly propagated to the \RV.
\item
\textbf{Fitting the observations incoherently:}
The power spectrum of p-modes can be Fourier transformed into a kernel
for a Gaussian Process.
This Process captures the time-correlations of the p-mode
radial-velocity variations across exposures taken at different times.
Because the integral of a Gaussian Process is also a Gaussian Process,
this kernel can be used to compute the correlations between exposures
taken for any time intervals at any time lags.
Thus it might be possible to fit any set of exposures with a bespoke
Gaussian Process, perfectly suited to capture the p-modes for each
star, including the coherent and incoherent effects all simultaneously.
There has been some work in other domains along this direction (\citealt{DFMGP}),
but it hasn't been applied to \EPRV\ and it is extremely
promising for p-mode mitigation.
\item
\textbf{Simultaneous temperature monitoring:}
It is likely that \RV\ variations from p-modes can be predicted
to some extent with stellar luminosity variations
(Sharon.~X.~Wang, private communication; also see related work on activity by \citealt{Aigrain}).
The \RV\ variations are expected to be related to the time derivative of the
luminosity variations.
As we note above, luminosity variations will be associated with
temperature variations, and at high signal-to-noise these may be
visible in \EPRV\ campaigns.
To find them, we might need to beat the observed spectra against an
expected temperature derivative at known p-mode frequencies.
That is, we might need to build a matched filter in the joint
two-dimensional domain of wavelength and time.
If we can detect the temperature variations, they will provide great
prior information about \RV\ variations and create another mitigation
strategy. 
This strategy will also be relevant to tackling phenomena like starspots in
the next phase of our proposal.
\end{itemize}

\noindent
Because we have access to proprietary \EXPRES\ data on $\tau$ Ceti 
(from Debra Fischer at Yale; see letter of support),
we can perform these experiments on existing, real data.
We don't have to make assumptions about how p-modes work.
And over the course of this project, more public data sets may appear
with the right properties at even higher precision from \ESPRESSO\ and \NEID.

If we find that mitigation strategies are best when exposures
are short, we will look seriously at the engineering trade-offs
of extra overheads (like read-out) and extra noise from taking
more exposures. These must be balanced in any optimized program
against benefits of mitigating the p-modes.

\textbf{The deliverables from this Second Stage (p-modes) will be ranked
and analyzed strategies and advice for \EPRV\ programs.} It will
also be methods and associated open-source code for each of the successful
mitigation strategies.
The expectation is that this Stage will produce at least one
refereed paper about p-modes. 
This paper will build upon the philosophy and concepts introduced in the First Stage 
papers and explicitly discuss the mitigation strategies tested in 
terms of information theory.

Together, the First and Second Stages set the scene for tackling more
pernicious sources of correlated noise in \EPRV.
These comprise the Third Stage.

\section{Third Stage: Mitigation of myriad variabilities (stochastics)}

The Third Stage is to consider the incoherent variabilities of stars
that are not well understood theoretically, or which imprint
stochastic time-variable signatures on both the spectrum and the
derived \RV\ measurements.
These include (but are not limited to):
\begin{itemize}
\item
\textbf{star spots:}
Star spots create effective \RV\ perturbations because they reduce the
emission from a part of the rotating stellar surface.
That affects \EPRV\ to zeroth order, but at first order there are other
effects of star spots:
They are localized in velocity space, and they have a different
temperature from the surface of the star.
Therefore they ought to be visible in the two-dimensional space of
spectrum and wavelength as a cooler stellar patch slightly more
localized in velocity than the whole stellar surface.
Our approach on star spots, building on prior work (\citealt{Gully}) will
be to consider lower-temperature spectral components with small
velocity shifts and try to fit everything simultaneously.
If the spots are reliably lower in temperature, it should be possible
to detect and mitigate them using physical photosphere models, at
least to first order.
This modeling is possible to implement as an extremely straightforward
extension of the \wobble\ framework. 
It also builds directly on the temperature-dependent experiments we will do
for stellar oscillations as part of the Second Stage.
\item
\textbf{plages, faculae, and other surface features:}
These features are similar to star spots and will imprint in similar
ways.
The main difference between star spots and other kinds of surface
features is that we don't have a good idea of the intrinsic spectral
properties of other kinds of features, which can show up as chemical
anomalies (at least in hot stars; \eg, \citealt{doppler}),
temperature anomalies (\eg\ \citealt{Gully}, \citealt{Milbourne2019}),
or mixtures of the two.
If we want to be flexible about surface features, we want to model
them as we propose for star spots, but permitting their spectral
properties to be determined by the data directly.
This is also possible as an extension of the \wobble\ framework.
Indeed it is very natural because in the current \wobble\ setup, the
star and tellurics are both modeled in a completely data-driven way. 
Also, notions of time coherence as explored in the First Stage will be 
relevant to this problem.
\item
\textbf{surface convection:}
Hot material wells up and cold material sinks on the surface of the star.
On average these motions cancel, but they don't fully cancel spectrally.
Thus there are convective blueshifts in stellar spectra.
But the variance of this process also ought to have spectral
properties, and also some time coherence because the convection
pattern is a smoothly varying, slowly rotating property of the stellar
surface.
Our hope is that even this extremely stochastic process will have
low-dimensional structure in the spectral domain (it's mainly
temperature, after all, and some non-LTE effects) and also have
valuable covariance structure in the time domain.
If our hopes are justified, it will be possible to fit a stochastic
process (like a Gaussian Process) to the time domain, in a low-rank
representation of the spectral domain.
That's speculative, but it is worth a look.
\end{itemize}

\noindent
We will not consider all possible sources of stochastic variability;
the project staffing and timeline are not enough!
We will prioritize around the intersection of team interests (and
especially GRA interests), opportunity relative to other community efforts
(to use our resources for highest impact), and opportunity in terms of
impact on \EPRV\ projects (amplitudes of signals, flowed down to final measurements).
Since we don't yet understand these variabilities in detail, part of this
proposal is to do the work to make those priority decisions.

We emphasize here that we are \textit{not} proposing to ``solve
all stellar variability.'' \textbf{What we are proposing to do is develop a set of 
principled, largely data-driven methods and statistical ideas, which may be 
broadly adapted to different \EPRV\ challenges.} This Third Stage is the 
point at which we use the knowledge gained in the First Stage (where we 
laid out an information-theory-based framework) and in the Second Stage 
(where we tackled a relatively well-understood and tractable example 
of stellar spectral variability) and apply it to more general and perhaps
less well-behaved sources of noise.

As we said in the Introduction, the key idea is that the form of the
data-driven models we build match the form of the variability that
we expect.
So, for example, when we build a model to capture star spots, we want
to give the model the structure that we expect to have in star spots,
but the flexibility to adapt to the data.
This might look like adding to the \wobble\ model extra spectral
components that have the flexibility to look like cooler star surface
spots, and which have velocities relative to the mean stellar surface
velocity in the range permitted by the stellar rotation ($v\sin i$).
It might additionally involve requiring observations that are close
in time to have spot components with similar velocities.
Such additional components could then be optimized along with the other
components of \wobble.
That's just an example, but shows the kinds of approaches we imagine taking.

Again, for this Third Stage (stochastics), we will do experiments on
\EXPRES\ data we have in hand, and
public \HARPS\ data from the Archive.
That is, all the data necessary for this Stage is already in place.
In addition, we expect more relevant data to appear in public archives
from the \ESPRESSO\ and \NEID\ instruments.
We will adapt to new data as it becomes available.
For some of these projects we could benefit from Solar
spectra, since the Sun is a great example of the kind of star we are
intested in; by construction, it is Sun-like!
And data can be taken with very good signal-to-noise and time resoluion.
There are both \HARPS-N and \HARPS\ data on the Sun that will become
public during the project duration. 
\EXPRES\ and \NEID\ will also likely obtain Sun-as-a-star data. 
So many excellent sources of data exist for our use in the Third Stage.

\textbf{The deliverables of the Third Stage (stochastics) will be papers in the
refereed literature about stochastic sources of stellar spectral
variability.}
These publications will also include numerical methods, open-source software, 
documentation, and tutorials, as with previous works published by the 
proposing team \citep[e.g.][]{Bedell2019}.
Dissemination will be through papers, conference participation, and
community workshops we run in Years 2 and 3 (more detail below).

\section{Community workshops}

Building on the \PI's and co-I's success with workshops like
\project{AstroHackWeek}s, the \project{Gaia Sprint}s,
\project{Telluric Line Hack Week} and \project{\TESS\ Hack Week}s
(see biographical sketches), \textbf{part of this proposal is to organize and
run two community workshops aimed at increasing community uptake of our
tools, and broadening statistical training in the \EPRV\ community.}
These workshops will run at the Flatiron Institute, where both the
\PI\ and co-I have facilities to run these workshops free of charge
(see the Facilities and Resources part of this proposal).
Each workshop will be 4.5 days of a week, and be a mixture of
pedagogical content, research presentations, and extensive time for
hands-on work (hacking) on participant research projects.
As with our \project{Telluric Line Hack Week} in 2019 February, we
will aim for 30-ish participants at each workshop, and we will theme
the workshops to attract advanced undergraduate students, graduate
students, postdocs, and early-career scientists who are actually
implementing the code and pipelines for their projects.

The schedule will involve some pedagogical and contributed presentations,
but most of the week will be ``unconferenced'', in which participants
decide democratically what to discuss, for how long, and when, and also
what times they want to actually download data and code and work.
This works extremely well, and we have been running meetings in this 
style for years now with good outcomes.
We even have a paper about one of the hack-week series (\citealt{AHW}).

Given that this is an \NSF\ proposal and we respect strongly the larger
goals of the \NSF, \textbf{we will work especially to involve
under-represented minority scientists, early-career scientists, women, 
and researchers from diverse backgrounds}.
We have had good success with this with the \project{AstroHackWeek} projects,
but we did find that doing well with URMs requires active work on the part
of meeting organizers, both in getting applications, and balancing the pool
of acceptances.

These workshops are critical to our success.
We know from our software projects---like \project{Astrometry.net},
\project{emcee}, and \project{wobble}---that you don't get customers
and user engagement just by putting your software out there.
You also need to help people engage with it.
We increased the number of users of \project{wobble} by an order of
magnitude in 4 days of active workshop!
This is partly because in a hacking-style workshop, people can actually
get the software working during the week itself.
This motivates a workshop that is not passively listening to talks, but actively
doing things relevant to participants' projects.

The content and style of the workshops is important because we want to deliver our tools and methods
to the widest possible range of scientists, from the widest possible set of
backgrounds; we want to meet people where they are, and we want
to have a positive, inclusive impact on the broader culture of our field(s).

\section{Timeline and project management}

Although the three parts of this project are presented as ``stages'',
they will not be staged serially.
Rather we will run all three in parallel to make best use of our
personnel and benefit most from the conceptual and data overlaps of the three stages.
All three stages will involve \PI\ Hogg, Co-I Bedell, and the project
graduate student student (\GRA). 

The First Stage (information) will be led by \PI\ Hogg, with experiments (the
toy-model experiments) performed by the \GRA, and writing by the full
team.
It will occupy parts of Years~1 and 2, producing a paper
in each of those two years.
One paper will focus on the information-theoretic approach to the problem,
and the second will focus on the causal-inference approaches.

The Second Stage (p-modes) will be led by Co-I Bedell, with
experiments performed by the \GRA, and writing by the full team.
The scientific analysis will take place in Years~1 and 2, with
writing and dissemination in Years~2 and 3.
This stage will produce one paper focused on direct comparison of 
passive approaches (nulling) and active approaches (resolving
and fitting) to treating the same data. 
The dissemination part will also include the two community workshops on
\EPRV\ as a time-domain problem, in which we build on our experience
of working with radial-velocity teams to bring new methods and
software practices to the community.
These workshops will happen in Years~2 and 3.

The Third Stage (stochastics) will have different components led by \PI\ Hogg,
Co-I Bedell, and the \GRA, with a schedule that depends on the
particular interests of the \GRA.
Each of the sources of stochastic variability will have different
signatures in time--spectrum space (as discussed above); we will use
Year~1 to do exploratory work in \wobble\ (\citealt{Bedell2019})
model residuals to set priorities
(which projects will be easier to see in the data, which harder).
That prioritization will create a schedule for projects, with data
analyses and experiments performed by the \GRA, and projects supervised
by \PI\ Hogg and Co-I Bedell as it makes sense.
Some of the exploratory experiments will produce publishable results
to write up in Year~2 and more holistic mitigation will require
methods and software that will be built in Years~2 and 3 and published
and disseminated in Year~3.
Again, we will use the community workshops in Years~2 and 3 as part of
the dissemination.

\PI\ Hogg will be responsible for overall project management. This is a
small-scale project; most of the management will involve decisions about our
priorities given the experiments we perform in Year~1 and the
challenges we discover there.
\PI\ Hogg will also be responsible for academic supervision of, and
career mentoring for, the \GRA.
The two workshops will be chaired by Co-I Bedell, with \PI\ Hogg and
the \GRA\ on the organizing committees.

One possibly odd thing about this project is that there are no budget lines for
the Co-I Bedell nor for the two community workshops.
These points are addressed explicitly in the Facilities and Resources part of this
proposal, plus in a letter of support from the Director of the Center for Computational
Astrophysics at the Flatiron Institute.
The Flatiron Institute is part of the Simons Foundation, which is not permitted to
accept or receive external funding from \NSF\ or any other body.
Therefore, Co-I Bedell cannot be paid on this grant and commits her time and
effort without any budget line from \NSF.
As the letter of support says, this is consistent with her appointment at the Flatiron
Institute.
In addition, both the Co-I and PI have signing authority to make use of
the Flatiron Institute's workshop and meeting facilities, including all amenities
associated with that.
All of this is discussed in the Facilities and Resources part of this proposal.

\section{Intellectual merit}

Finding Earth-like rocky planets on year-ish orbits
around Sun-like stars (also known as ``Earth 2.0'') is one of the most fundamental goals
in contemporary astrophysics.
It connects to the origins of the Sun and Solar System.
And it connects to the origins of life on Earth!
Understanding our place (Earth, humans) in all this is critical for physics, for science, and for the
public, who are curious about this above almost all other things.
And the astronomical community is rallying around this goal: many new \EPRV\ instruments 
are coming online, and NSF and NASA are jointly exploring future possibilities via the 
\project{\EPRV\ Working Group} (in which Co-I Bedell is a participant).

These significant investments in \EPRV\ \textit{will not meet their goals} if
stellar noise remains a limiting factor at the $1\,\mps$ level. 
We propose to build critical understandings, tools, and methods that will transform
these projects into something that \textit{does} have a shot at making the field-changing 
discovery of an Earth-like exoplanet. (Or better yet, a statistically significant sample of them!) 
We are proposing to make every instrument more sensitive, every star more useful,
and every observing campaign more efficient.
We might end up delivering critical technology for the discovery of Earth 2.0.

In addition, we expect to learn a lot about stars themselves in this project.
We will effectively see sunspots on stellar surfaces, and build effective models
of surface convection.
We will be sensitive to asteroseismic mode amplitude and phase variations, which
are in principle measurements of the properties of stellar interiors.
An \EPRV\ measurement has a star at one end and a spectroscopic instrument at the other.
The \EPRV\ community has become remarkable at calibrating their \emph{instruments}.
Here we are taking a serious shot at calibrating their \emph{stars}. 
The resulting science will be relevant to both the exoplanetary and stellar astrophysics 
communities.

\section{Broader impacts}

This project brings together ideas from statistics, information theory, applied mathematics, machine learning, stellar structure, and astronomy.
Operating at this multi-phase interface, this project creates interdisciplinary graduate education and training that is unique in the exoplanet community.
The \GRA\ supported by this project will be a trained member of the \acronym{STEM} workforce in timely capabilities that go way beyond academic astrophysics.

Relatedly, this project, the papers it produces, and the workshops we run, will be used to propagate some of this interdisciplinarity to early-career scientists from diverse institutions and backgrounds.
We will be helping to broaden the training of many early-career scientists.
And we will be helping those early-career scientists to reach their intellectual and career goals more efficiently and with better tools.
It will also broaden their career horizons, since the interaction with methodological domains is important in many career trajectories both inside and outside academia.

As part of this project, we are committing to advertise our workshops broadly and admit participants with diversity and inclusivity and integration in mind. 
We expect to succeed here, based on our experiences with the workshops we have run previously.
However, because these inclusivity desiderata are explicit goals of the workshops we run, we believe that this work will bring new ideas and new practices to our home institutions.
Anything we learn, implement, and establish at these institutions will affect workshops and activities and other kinds of institutional choices going in to the future.
That could deliver a very broad impact.

Finally---and somewhat uniquely in this community---we operate in an entirely transparent and open way.
Even this proposal can be read on \project{GitHub} along with all the edit history!
We operate this project with public data, and we create open-source code, and we deliver public
high-level data products.
We create educational and outreach opportunities directly by this openness.
One of the broader impacts of this project is that we help enable new investigations by anyone, inside astronomy or out.

\section{Prior NSF support}

\paragraph{\acronym{AST-1517237}
\project{New Probabilistic Methods for Observational Cosmology}
(2015--2019) \$328K:}
This project supported PhD dissertation work at NYU by Alex Malz.

\paragraph{Intellectual merit:}
In this project we developed probabilistic methods for dealing
with noisy and probabilistic outputs from large-scale structure surveys,
especially surveys that rely heavily on photometric redshifts and other
kinds of noisy and contaminated data.
The methods produced by the project make future projects like \LSST\ more
precise and more capable.

Because the methods are general, we also, as part of this grant,
applied similar and related methods to exoplanet discovery.
These included publications in which we calibrate \Kepler\ light curves,
detect planets in the \Ktwo\ Mission data, and infer the rate at which
Sun-like stars host Earth-like planets.
Some of the methods we developed have been used in analyses of the
\NASA\ \Kepler\ and \TESS\ Mission data.
We also applied related and derived methods to stellar spectroscopy,
and to the stellar populations found in the \ESA\ \Gaia\ data.

\paragraph{Broader impacts:}
The grant proposal was co-written by Dr Malz and established his career.
During the award period, Malz brought many statistical innovations into the
cosmology \LSS\ community, and had a big impact on the \NSF-funded \LSST\ Collaboration.
The proposal also supported the creation and publication of several pedagogical contributions
on the arXiv which are used for training advanced undergraduates and graduate students.
The grant supported travel by Malz and Hogg to American Astronomical Society meetings
to organize, participate in, and run \project{Hack Together Day} and travel to
\project{AstroHackWeek} to organize, participate in, and run those meetings too.
Malz helped organize hack days inside the \LSST\ Collaboration as well.

\paragraph{Publications and products:}
The publications supported the following publications:
\citet{2016arXiv160303040C, 2016arXiv161005873V, 2016ApJ...833..262H,
  2017ApJ...837...20P, 2017ApJ...838....5L, 2017AJ....153..257O,
  2017MNRAS.469.2791H, 2017arXiv171002428W, 2017MNRAS.471..722H,
  2017AJ....154..222L, 2017MNRAS.472.4144V, 2018ApJ...853..198N,
  2018ApJ...857..114W, 2018arXiv180407766H, 2018ApJS..236...11H,
  2018AJ....156...18P, 2018AJ....156...35M, 2018AJ....156..145A,
  2018ApJ...867..101B, 2019MNRAS.482.3696C, 2019ApJ...872..115V,
  2019ApJ...881...80L, 2019arXiv190811523D, 2019AJ....158..171M}.
It also supported open-source code, published on \project{GitHub}.

\paragraph{Other \NSF\ support:}
In addition to the highlighted grant, the \PI's group has been supported by
\acronym{OAC-1841594}
\project{Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics};
\acronym{IIS-1124794}
\project{A Unified Probabilistic Model of Astronomical Imaging};
\acronym{AST-0908357}
\project{Dynamical models from kinematic data: The Milky Way Disk and Halo};
\acronym{AST-0428465}
\project{Automated Astrometry for Time-Domain and Distributed Astrophysics}; and
\acronym{PHY-0101738}
\project{Theoretical Particle Physics, Astrophysics and Cosmology}.
The majority of the \PI's 220 refereed publications have been at least partially supported
by the \NSF.

\section{Not-so-frequently asked questions}

\paragraph{Why do this now?}
There are a few extremely precise spectrographs operating now, 
and many more on the horizon \citep{Wright2017}.
\EPRV\ is entering a golden era, brought on in part by missions like
\TESS, which require follow-up, and in part by a hope that
\EPRV\ can detect the true Earth analogs (long-period, low-mass, rocky
planets around G-type stars).
If we don't solve these stellar spectral-variability issues, these
projects will not live up to their greatest promise.
And the time-scale of this grant is matched to the
commissioning of new instruments.

\paragraph{Isn't this proposal a giant fishing expedition?}
Kind of! It's true that we (and the community in general) are not certain 
which aspects of variability are most critical, and which 
methods are best suited. We are not promising to 
``solve'' stellar activity or stellar spectral variability in this proposal.
Much of our work will be 
experimental. But this proposal is more than a series of experiments. 
We believe that overcoming stellar variability and 
discovering Earth analogs will require data and effort from multiple, 
diverse \EPRV\ instruments, techniques, and groups. To this end, we will establish 
a common language and theoretical framework 
to guide the community. Through the papers produced in this 
program and through ``hack-week''-style events hosted by us, we aim to 
lay the groundwork that \textit{will} solve the problem of stellar variability 
in \EPRV, regardless of the success of each individual experiment we do. 

\paragraph{Isn't this work already being done?}
It certainly is true that the community is working hard on time
variability in some \EPRV\ contexts.
However, most of the work (with very notable exceptions) has centered
around stellar activity, and most of the work has centered on the
cross-correlation function.
It is our view that this is such a tall pole for \EPRV\ that we need
to launch and support multiple groups working from multiple angles.
We will be different from the community in a number of ways, but one
is that we are considering fully data-driven approaches, and in the
full outer-product-space of time and wavelength.
We also have a conceptual bent: We will try to change the conversation
about \EPRV\ and then build mitigation strategies that talk to the
new frameworks and languages.
This does not represent any duplication of effort.

\paragraph{You can't ever really get rid of stochastic stellar noise, right?}
If something is called ``noise'' we tend to think ``oh well, it's
noisy''. But if noise is structured---in spectral space or temporal
space or both---then it can be modeled continuously.
This proposal argues that all the important noise sources in \EPRV\ coming
from spectral variability will be structured in both time and wavelength.
Therefore there are handles for modeling and mitigation.
We are optimistic that these approaches will work, at least substantially.

\paragraph{What is the metric for project success?}
Given that we're \textit{not} claiming that we'll 
solve astronomy or find Earth 2.0 in this project, this is a reasonable 
question! Our metric is twofold: one, we will know we have succeeded when we 
produce papers and accompanying open-source code as outlined in the Timeline. 
Secondly, and more subtly, we will know we have succeeded when we have engaged 
the \EPRV\ community to build upon our work and use the statistical framework 
laid out in our papers as a launching pad for further breakthroughs. 
In practice, this will look like broad and diverse attendance at our workshops; 
citations to our papers; and maybe, eventually, the discovery of Earth 2.0
using our techniques!

\clearpage
\bibliographystyle{apalike}
\raggedright
\bibliography{description, prior}%general,myref,inprep}

\end{document}
