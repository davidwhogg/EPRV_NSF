% This file is part of the SolveAllOfAstronomy project.
% The GetRichOrDieTrying project is a different thing entirely.
% Copyright 2019 David W. Hogg and Megan Bedell

% # Style notes
% - Third person mainly (but maybe not exclusively -- use first for emphasis!)
% - Use macros for acronyms, project names, and so on!

% # To-do list
% - Do we need to explain wobble in greater detail?
% - Choose more or different sentences or phrases to \textbf{}

\documentclass[12pt, letterpaper]{article}
\usepackage{fancyhdr, varioref}
\setlength{\headheight}{3ex}
\setlength{\headsep}{2ex}
\input{hogg_nsf}
  \renewcommand{\headrulewidth}{0pt}
  \pagestyle{fancy}
  \lhead{\textcolor{darkgrey}{\textsf{\acronym{CDS\&E}: Precision measurement of stellar \RV s in the presence of intrinsic variability}}}
  \rhead{\textcolor{darkgrey}{\textsf{\thepage}}}
  \cfoot{}
\begin{document}\sloppy\sloppypar\raggedbottom\frenchspacing

\noindent
In many important regimes, the detection and characterization of exoplanets
requires extreme precision radial-velocity (\EPRV)
measurements.
With \EPRV\ programs we can detect planets in the medium-to-long 
orbital-period regime that is under-represented in transit searches.
We can measure the 
masses (and hence compositions) of transiting planets,
and we can characterize host stars and systems in exquisite detail.
With the next generation of \EPRV\ 
instruments
we hope to detect terrestrial planets on Earth-like orbits.

From a fundamental-physics perspective, \emph{there is essentially no limit to how
precisely astronomers might measure the radial velocity variations
of the center of mass of a distant star with a spectrograph}.
Of course in real life, there are real limits, set
by (for example) photon noise, stellar surface convection and activity,
variations in the Earth's atmosphere, and spectrograph calibration stability.
In practice, almost no stars \emph{reported in the literature}
have had their center-of-mass
velocities measured consistently with an empirical scatter
smaller than $\approx 100\,\cmps$ 
($1\,\mps$). 
There are rumors that the \acronym{ESO} spectrograph
  \ESPRESSO\ \citep{Pepe2010} and the Yale spectrograph \EXPRES\ 
  \citep{Jurgenson2016} are seeing 
  30-ish $\cmps$ around certain very quiet stars.
This is very promising but unlikely to hold true for the vast majority of stars, or even for these targets during different phases of their activity cycles.

At the same time, \EPRV\ is responsible for hundreds of \foreign{ab
initio} exoplanet discoveries and hundreds more \foreign{a
posteriori} characterizations of planets discovered elsewhere.
That is, \EPRV\ is hugely successful.
Because most of the discoveries and characterizations have been made
at or near precision limits---that is, near what is detectable at noise levels
greater than $1\,\mps$ per epoch---\textbf{any improvement in precision directly
translates into increased scientific return on investment},
both from existing archival data and from new data from new programs.

\paragraph{Stellar variability is the ``tall pole'':}
Right now the best spectrographs are obtaining internal calibration
consistencies of a few $\cmps$ \citep{espresso-eprv4, expres-eprv4}.
These internal precisions are determined by repeatability and
stability of calibration references such as arc lamps, etalon
(Fabry--Perot) interferometer signals, gas cells, or laser-frequency
combs.

It is therefore now widely accepted (and convincingly
established to us) that the majority of the radial-velocity variance
in \EPRV\ campaigns is coming from the stars themselves, or what's
often called ``astrophysical noise'' (convection, star spots,
asteroseismic pulsations, plages, faculae, and flares, for example).
This proposal is, therefore, \textbf{to address the dominant sources of
  noise in \EPRV\ experiments and to create working methods to
  mitigate them}.
You can precisely calibrate the variations in the spectrograph.
Here we ask: Can you also precisely calibrate the variations in the star?
We believe that the answer is \emph{yes}.

There is also noise coming from telluric lines in the Earth's atmosphere
(especially unmodeled tiny micro-tellurics).
In a separate but related project \citep{Bedell2019} we are addressing this
source of noise as well.
Our work suggests that tellurics and micro-tellurics are addressable problems
and do not currently dominate the end-to-end noise budget.
But it is certainly also an important area for work and study.

\textbf{Our approach to stellar variability is to build data-driven models of what
we don't understand, but with model structure carefully constructed to
match what little we \emph{do} understand.}
That is, to build flexible models which have their capacity tuned to
capture the variations we expect to see. 
Such models will capitalize on our physics-based expectations to
maximize interpretability, while simultaneously having the flexibility to absorb 
the ``unknown unknown'' effects of stellar variability.
%The proposers have strong cred in this area.

\paragraph{Why this team?}
The proposing team has a demonstrated record of expertise in making
measurements of stars at the limits of observational precision. 
PI Hogg has built novel data-driven models of the color--magnitude diagram of
stars, improving ESA \Gaia\ parallax measurements (\citealt{Leistedt, Widmark, Anderson}).
These models re-purpose machine-learning tools to build models that
have causal structure useful to astronomers (they know about
luminosity distances and parallaxes, for example).
Co-I Bedell has developed new techniques to make exceedingly
precise measurements of stellar element abundances with minimal
reliance on stellar models, by designing perfectly differential stellar
comparisons \citep{Bedell2014, Bedell2018}.
PI Hogg has the current best-in-class methods for determining stellar
parameters and abundances from survey-quality spectroscopy (\citealt{Cannon2, Doppel}).
These methods (called \project{The Cannon} after Annie Jump) are
data-driven but designed to capture the smooth variations (with
respect to parameters) that we expect in stellar spectra.

And most relevant of all, \textbf{PI Hogg and Co-I Bedell built an open-source
pipeline for \EPRV\ data analysis} (\wobble; \citealt{Bedell2019}).
This pipeline has many novel aspects to it, but some of the key ideas
are the following: 
It separates stellar and telluric signals in the data in a purely
data-driven way.
It delivers Fisher-information-saturating \RV\ measurements without any
theory-derived or external stellar template; it gets the stellar model
almost losslessly from the data themselves.
It makes use of \project{TensorFlow} and ideas from convex optimization
for computational tractability and performance.
It permits telluric variability and can be extended to permit stellar
variability (that's this proposal).
And it is extensible, flexible, and open source, so that it can be
tweaked and used by different teams with different harware and goals.

In addition to building this software, the proposing team is known for
convening and running hack-week-style workshops (such as \project{Gaia
  Sprint}, \project{Preparing for TESS}, and \project{Telluric Line
  Hack Week}) in which participants learn to use new data sets, new
data-analysis methods, and new software for their scientific goals.
Since a big part of this proposal is dissemination and community adoption,
this track record is very relevant to the current proposal.
And for the long-term value and sustainability of the open-source methods
and software we develop.

Finally, this team has close ties to multiple \EPRV\ instrument and survey
teams, including the \project{Terra Hunting Experiment} and the \project{EXPRES}
spectrograph. Together with Co-I Bedell's extensive experience with utilizing 
publically-available \project{HARPS} data, these connections ensure that 
we can obtain the data sets we need, and that the findings we produce will 
be noticed by the \EPRV\ community.

\paragraph{Relevance to \NSF\ programs and objectives:}

HOGG, BEDELL: TBD

BEDELL: RELEVANCE TO AAG (email with Neff)?

HOGG: RELEVANCE TO \textbf{CDS\&E}

HOGG: NSF BIG THEMES?? The \textit{\NSF's 10 Big Ideas} document
includes two themes of relevance, which are \textbf{Growing
  Convergence Research} and \textbf{Mid-scale Research
  Infrastructure}.

HOGG: Is \textbf{Understanding the Rules of Life} also relevant?

\paragraph{This proposal, in a nutshell:}
Stellar variability is the tall pole in \EPRV\ at the present day.
We propose to build customized, data-driven models to capture and mitigate these
stellar astrophysical noise sources, taking advantage of modern advances in
data science and applied math.
Our driving motivation is to make \EPRV\ measurements an order of magnitude more precise
and thereby transform the science
that can be done with the current and next generation of spectrographs. 
Along the way, our contributions to the literature will 
establish a common statistical framework to enable communication 
and methodological comparisons across the global \EPRV\ community. 

\subsection{Intellectual merit}

HOGG: TBD

\subsection{Broader impacts}

HOGG, BEDELL: TBD

\section{First Stage: Bounds on \EPRV\ in time-varying stars (information)}

Perhaps the strangest thing about our science goal is that
it is \emph{conceptually impossible}!

Okay, not really, but it is worth remembering that the entire
\EPRV\ program is built on the premise that you can measure
\emph{relative} velocities far more precisely than you can measure
\emph{absolute} velocities, because the relative velocity between two
exposures of the same star is seen as a spectral shift from one
exposure to the next.
This point depends on the assumption that the spectrum does not vary
between exposures!
Or that it doesn't vary \emph{adversarially} between exposures.

\hoggbox{\paragraph{Information Theory:}
The fundamental limit on \EPRV\ precision is given by the Cramer-Rao Bound, which is
the Fisher Information in this case.
The Fisher Information is the best possible inverse-variance that can be obtained
on any \RV\ measurement, and it appears in the literature in multiple places,
although not usually written in this language \citep{Butler1996, Bouchy2003}.
Symbolically, in the case of a time-invariant spectrum,
the Fisher Information $\sigma_v^{-2}$ is given by
\begin{equation}\label{eq:fisher}
\sigma_v^{-2} =
\left[\frac{\dd f}{\dd v}\right]^{\mathsf T}\cdot C^{-1}\cdot
\left[\frac{\dd f}{\dd v}\right] \quad ,
\end{equation}
where $\sigma_v$ is the \RV\ uncertainty, the derivatives are the derivative
of the spectral expectation $f$ (seen as a column vector) with respect to \RV\ $v$,
and the inverse variance of the noise in the spectrum is $C^{-1}$ (diagonal in the
simple case that all spectral pixels are independently measured; more complex in
what follows!).

Things get a bit more complicated when there are nuisance parameters, but
not fundamentally different: The derivatives
become rectangular projection operators and the Information becomes a tensor (matrix)
must be inverted to deliver nuisance-marginalized uncertainties.

When we turn on stellar variability, there will be two ways to modify the Fisher
Information.
In one framework, the stellar variability is seen as an additional source of noise
in the space of wavelength and time.
The Information gets modified by having $f$ become not just one spectrum but a
full time series of spectra, and by having $C$ become not just the variance tensor
of the instrumental noise, but also of the ``noise'' associated with the spectral
variability (which will be covariant in both wavelength and time).
The additional noise will increase the eigenvalues of $C$, decrease the eigenvalues
of $C^{-1}$, and decrease the precision (increase $\sigma_v$).

In another framework, the stellar variability is seen as an additional set of
nuisance parameters (for example, the amplitudes of eigenspectral contributions
to the spectrum at each observation).
In this framework, the stellar variability is seen as a set of nuisance parameters
that add dimensions to the Information tensor.
This will also serve to reduce the precision (increase $\sigma_v$).
}

Once the spectrum is permitted to vary between exposures (and we know that
it does), the fundamental assumptions of the \EPRV\ literature break down.
In the First Stage (information) we propose to fix this problem, by
working out the conceptual framework for thinking about
\textbf{information theory in \EPRV\ in the presence of a variable
  spectrum}.
We have identified a few approaches to this problem, and we intend to
work down all paths.
This First Stage will produce papers from each of the paths that proves
to deliver useful results or frameworks.
This is the \emph{theory part} of this proposal.

The question is: How precisely can a stellar RV be measured in the context
of stellar spectral variability?
Of course the answer depends on exactly how the star is varying;
in detail it depends on questions about the variability like the following:
\begin{itemize}
\item
\emph{What is the amplitude of the variability?}
If the amplitude is tiny, it won't strongly affect \RV\ measurements.
\item
\emph{What is the dimensionality of the spectral variability?}
If the spectrum varies in a low-dimensional space spanned by a small set
of eigen-spectra, \EPRV\ measurements can be made in the tangent space
orthogonal to the spectral variability.
\item
\emph{What is the time coherence of the variability?}
If the variability is uncorrelated from observation to observation, it
can be mitigated by repeat exposures. If it isn't, more complicated
strategies are required.
\item
\emph{What is the projection of the spectral variability onto the RV derivative?}
The most adversarial kind of spectral variability is that which looks
like the derivative of the spectrum with respect to \RV.
\end{itemize}

\noindent
All of these issues will come up again below in the Second Stage (p-modes)
and Third Stage (stochastics).
But in the First Stage, we want to understand precisely and quantitatively
how these issues (amplitude, dimensionality, time coherence, and projection
onto RV derivative) affect fundamental limits on the precision of
\EPRV\ measurements.
That is, in the First Stage we are trying to understand what is theoretically
possible for \EPRV.

There are four approaches we will take in the First Stage (information)
to answering these theory questions,
and the papers we write in the First Stage depend on our success with each
approach, and how appropriate each approach is, given the magnitudes, spectral
shapes, and time structures of the noise sources we find in the data (and in
the Second and Third Stages). 
These four approaches are the following:
\begin{itemize}
\item
\textbf{Pure noise model:}
If the stellar spectral variability has low amplitude, and either has
no spectral or time structure, or else we are pessimistic about
modeling it in any way, we face the \emph{worst-case scenario}.
In this scenario, all we can do is add the spectral noise to the photon noise
in the noise tensor $C$ in the Fisher Information definition (\ref{eq:fisher}),
in the box \vpageref{eq:fisher},
maintaining the spectral correlations in the noise (which are important),
and re-compute the Information in this bad context.
The Fisher Information will get smaller as the stellar variability
gets worse, and the resulting theoretical \EPRV\ uncertainties will
get larger.

In this approach, the idea would be to estimate (either empirically or
theoretically) the excess variance (in spectrum space) induced by the stellar
spectral variability.
This estimation could be performed with something like PCA, or a more
sophisticated latent-variable model (\eg, that of \citealt{HMF}).
That excess variance would be noise that is covariant in wavelength space,
but which could be added directly to the instrument (photon and calibration
and tellurics) noise tensor $C$ in the Information tensor (\ref{eq:fisher}).
The goal of this approach would be to demonstrate how an investigator
can estimate quantitatively the impact of various kinds of observed or
theorized stellar variability, if the investigator has an estimate of
its variance tensor in the observed space.
\item
\textbf{Spectral filtering:}
If the variability in the spectral domain is low-dimensional, then it is
possible to project the data into a space that is orthogonal to the
variability subspace.
This projection will cost some information, but it will return a time-invariant
spectrum and return us to the utopia of the time-invariant Fisher Information
calculation, just with a new noise model.
This approach is appropriate in a well-defined limit of variability (related
to dimensionality) and is
promising so long as the variability subspace does not contain (or does not come very
close to containing) the derivative of the spectral expectation with respect
to \RV\ (the derivative in the Fisher Information in the box \vpageref{eq:fisher}).

In this approach, the idea would be to estimate the (hopefully
low-rank) subspace (eigenspace) in which the stellar spectrum varies.
Once again, this could be estimated with PCA or more sophisticated
methods.
Then the idea is that at each epoch, the spectrum of the star is
described by a mean spectrum, a \RV, and a set of amplitudes with
which the eigenspectra co-add to fit the individual spectrum taken at
that epoch.
These amplitudes become nuisance parameters which complexify the
Information computation, as described in the box \vpageref{eq:fisher}.
In general the existence of this subspace will reduce the expected
\RV\ precision.
However, the amount that it reduces it will depend most strongly on
the projection of the \RV\ derivative of the mean spectrum onto the
subspace.
This method is called ``filtering'' because this information-theoretic
operation is equivalent to projecting the data onto a subpsace that is
tangent to the spectral-variability subspace.
That will look like a linear filter of the data.
\item
\textbf{Time-domain process priors:}
Variabilities can be incoherent (most are) or coherent (as p-modes are
over short time scales).
However, in long data sets (years), no source of stellar spectral
variability will look identical to a ``Kepler process'', or the time
dependence of \RV\ induced by mixtures of two-body orbits.
Thus, even in the presence of pretty adversarial stellar spectral
variability, \EPRV\ precision can be reobtained by observing for long
times, and separating the final \RV\ measurements over time into those
components that can and cannot be generated by a Kepler process.
These approches work well in certain limits, and work best when we can
understand the time correlations of the spectral variability quantitatively,
or when those time correlations are on time scales far from the
orbital times of interest.

In this approach, the idea would be to put a parameterized, rigid
prior on the exoplanet-induced signals (that is, that they are produced
by an N-body Kepler system), and a stochastic prior (like a Gaussian
Process) on the stellar-variability-induced signals, treating the
\RV\ measurements as being created by the sum of these two
processes.
Or even treat the spectral variability (in wavelength space) as being caused by these two
processes (if we want to treat the \RV\ shift as a variability).
This approach would require estimation of the statistical properties
(time coherence or covariance as a function of time lag) for the
\RV\ signals induced by the stellar spectral variability.
These can be estimated with statistics that look like auto-correlation
functions.

The information-theoretic question would then be asked in the space
of the Kepler parameters instead of the \RV\ measurements themselves.
And the Gaussian Process would become part of the nuisance space of
the problem.
This would require a substantial reformulation of the information
theory written in the box \vpageref{eq:fisher}, but it is not
difficult in principle.
It would end up answering a different question, however:
We would not ask ``how well can we measure \RV?''; we would ask
``how well can we learn Kepler parameters?'' (work along these lines
is underway also in the E.~Ford group at PSU, private communication).
This is the best approach if the stellar variability \emph{does} look
like or contain within its subspace the derivative of the mean spectrum
with respect to \RV.
That is, this is the approach for worst-case scenarios.
In non-worst-case it is still useful, however, because this approach
can be combined with the other approaches in this First Stage (information) to deliver
information-theoretic limits on Kepler parameters in the face of
different kinds of spectral variability.
We also propose to look at that question.
\item
\textbf{Causal modeling:}
Finally, for long-term stellar monitoring campaigns, there is an idea
from causal inference that can help us:
If the stellar intrinsic variability is uncorrelated with the orbits
of any planets (and we expect this except for extremely short-period,
locked planets), then there is no sense in which the spectral
variations should be able to \emph{predict} the \RV\ variations that
are planet-induced.
That is, a regression might be able to separate out the intrinsic
stellar variability.
This approach will only work reliably in projects performing very
long-term stellar monitoring, because it is justified only in a hard
limit.
However, there are many such projects starting now.
(This method is conceptually related to things we have done with
\Kepler\ and \Ktwo\ for planet discovery; \citealt{DFMK2, CPM, CPMdiff}.)

This approach is similar to the previous approach (time-domain processes),
but we determine the impact of the variability on the \RV\ measurements
by considering the extent to which the stellar spectrum variations
associated with the induced \RV\ measurements can be used to predict
the \RV.
The idea is to perform regressions and look at the predictive power, and
turn statistics of that predictive power into a quantitative estimate
of the impact on \EPRV\ precision.
The impact will be related to the covariance of the spectral variation
and the \RV\ variation; that is, it will be possible to estimate it
empirically.
\end{itemize}

\noindent
As we work down these approaches, we will develop results and assemble them
into something like two papers on the theory of \EPRV\ in the presence of
stellar spectral variability. \textbf{These two papers, and the associated open-source
code we generate to perform the justifying experiments, will constitute the
key deliverables of this First Stage (information).}
However, this Stage will also deliver advice and methods to
\EPRV\ investigators.
These methods will permit investigators to determine the impact of
various kinds of empirically observed stellar variability on their
end-to-end expected precision,
and help them to understand whether their end-to-end results are
achieving the \RV\ precisions they ought to expect.
\textbf{That is, we will deliver methods for estimating variability, and methods
for estimating the quantitative impact of those variabilities on \RV\ precision.}

One extremely important point, which we want to make in the papers we write
in the First Stage, is the following:
In the case that the stellar spectrum does not vary, the
cross-correlation function (\CCF) between the observed spectrum and a
(very good) spectral template (model spectrum) is a \emph{sufficient
  statistic} for the \RV\ measurement.
That is, the \CCF\ contains all of the \RV\ information in the data.
This is \emph{no longer true} once the spectrum starts to vary!
That means that approaches for dealing with stellar spectral variability
that are confined to considerations about the shape of the \CCF\ are
doomed to fail---or at least doomed to under-perform.
\textbf{Once the spectrum starts to vary, we have to adopt approaches that model
the data in the full two-dimensional space of photon wavelength and time.}
One of our primary high-level goals is to move the community in the
right direction on this point.

\section{Second Stage: Mitigation of asteroseismic variability (p-modes)}

Asteroseismic p-modes deliver a significant source of contamination in \EPRV\
measurements. 
As the star pulsates, its surface sinusoidally moves towards and away from
the observer, imparting Doppler shifts at the $\mps$-level or worse.
Traditionally these oscillations are dealt with by ``integrating over'' them
with a 15-minute exposure, but recent works have cast doubt on the efficiency 
of this approach \citep{Medina2018, Chaplin2019}. 
While the best methods for dealing with p-modes in \EPRV\ have yet 
to be convincingly established (and we intend to make progress here),
we can regard this noise source as a kind of
\textit{best-case} scenario for correlated noise. That is, we believe that 
p-modes can be nearly completely mitigated through spectral and temporal modeling, 
because they have two important properties:
The first is that
\textbf{p-modes are temporally coherent} on reasonable time scales (days to months, depending on
stellar type). Therefore, they can be fit as sinusoids over time intervals 
short relative to the coherence time. Furthermore, the frequencies are known,
or can be known, through monitoring (with the \EPRV\ spectrograph itself 
\citep[][and references therein]{Bouchy2003},
or photometrically with a \TESS-like mission).
The second important property is that
\textbf{p-modes have associated temperature signatures}. Indeed, the star gets
hotter when it contracts and cooler when it expands, adiabatically. Although for
G-type stars these temperature variations are tiny, they ought to be visible in
high-signal-to-noise \EPRV\ spectral observations.

These nice properties of p-modes, and the relatively straightforward physical 
interpretation of these oscillations, make them an ideal laboratory for testing
various stellar noise mitigation strategies. \textbf{The goal of the Second Stage is 
two-fold: we want to produce recommendations for optimal treatment of p-modes
in \EPRV\ measurements, but we also want to use p-modes as an 
example of how to work through correlated stellar noise from within the 
frameworks we established in the First Stage publications.}

(BEDELL FIGURE:
caption: Realistic p-mode oscillations for a Sun-like star arising 
from a series of damped driven simple harmonic oscillators at 
a forest of frequencies. We will use this simulation along with
real data to explore various mitigation strategies.)

We plan to explore four mitigation strategies for p-modes. We will stage 
illustrative experiments using a toy model of a Sun-like oscillator. 
We will also follow up using real data, mainly from short-exposure 
\EXPRES\ observations of $\tau$ Ceti. Both of these data sets are already
in-hand. The mitigation strategies we will explore are:
\begin{itemize}
\item
\textbf{Choosing resonant integration times:}
There is hope in the \EPRV\ community that cleverly chosen integration
times can obviate p-mode noise \citep[see, \eg][]{Chaplin2019}.
Current work in this area makes somewhat na\"ive assumptions about the
stability of the p-mode amplitudes. 
Using our realistic toy model of stochastic p-mode oscillations, we will
test different integration times. 
We will also experiment with binning the \EXPRES\ data to simulate 
different integrations. 
We expect to find that although some exposure times are better than
others, we need to actively mitigate, not passively avoid, p-mode
noise.
\item
\textbf{Resolving modes and fitting them coherently:}
The alternative strategy to p-mode avoidance is to take short exposures,
short enough that they Nyquist-sample the highest-frequency p-modes of
the star.
Any (say, 30-minute) visit to the star could be split into short (say,
2-minute) exposures.
This incurs some observing and read-noise overheads, but permits
resolution of the p-modes directly.
Then the obtained \RV\ measurements can be fit with a sum of coherent
sinusoids, regularized with the (presumed known) power spectrum of the
star.
That is, the amplitudes permitted for the coherent sinusoids would be
forced by regularization (or priors) to be consistent with the
expected p-mode spectrum.
Then the stellar \RV\ measurement relevant to exoplanet detection and
characterization could be corrected for the state of the p-mode
sinusoids during the exposure set.
\item
\textbf{Fitting the observations incoherently:}
The power spectrum of p-modes can be Fourier Transformed into a kernel
for a Gaussian Process.
This Process captures the time-correlations of the p-mode
radial-velocity variations across exposures taken at different times.
Because the integral of a Gaussian Process is also a Gaussian Process,
this kernel can be used to compute the correlations between exposures
taken for any time intervals at any time lags.
Thus it might be possible to fit any set of exposures with a bespoke
Gaussian Process, perfectly suited to capture the p-modes for each
star, including the coherent and incoherent effects all simultaneously.
There has been some work in other domains along this direction (\citealt{DFMGP}),
but it hasn't been applied to \EPRV\ and it is extremely
promising for p-mode mitigation.
\item
\textbf{Simultaneous temperature monitoring:}
It is likely that \RV\ variations from p-modes can be predicted
to some extent with stellar luminosity variations
(Sharon.~X.~Wang, private communication; also see related work on activity by \citealt{Aigrain}).
The \RV\ variations are expected to be related to the time derivative of the
luminosity variations.
As we note above, luminosity variations will be associated with
temperature variations, and at high signal-to-noise these may be
visible in \EPRV\ campaigns.
To find them, we might need to beat the observed spectra against an
expected temperature derivative at known p-mode frequencies.
That is, we might need to build a matched filter in the joint
two-dimensional domain of wavelength and time.
If we can detect the temperature variations, they will provide great
prior information about \RV\ variations and create another mitigation
strategy.
\end{itemize}

\noindent
Because we have access to proprietary \EXPRES\ data on $\tau$ Ceti 
(from Debra Fischer at Yale; see letter of support),
% public \HARPS\ asteroseismic campaigns for 
%several seismic stars including $\beta$ Hyi, $\alpha$ Cen B, and 18 Sco,
we can perform these experiments on existing, real data.
We don't have to make assumptions about how p-modes work.
And over the course of this project, more public data sets may appear
with the right properties at even higher precision from \ESPRESSO\ and \NEID.

If we find that mitigation strategies are best when exposures
are short, we will look seriously at the engineering trade-offs
of extra overheads (like read-out) and extra noise from taking
more exposures. These must be balanced in any optimized program
against benefits of mitigating the p-modes.

\textbf{The deliverables from this Second Stage (p-modes) will be ranked
and analyzed strategies and advice for \EPRV\ programs.} It will
also be methods and associated open-source code for each of the successful
mitigation strategies.
The expectation is that this Stage will produce at least one
refereed paper about p-modes. 
This paper will build upon the language introduced in the First Stage 
papers and explicitly discuss the mitigation strategies tested in 
terms of information theory. (HOGG: do you agree? are there edits we
can make to previous parts of this section to call out First Stage 
connections more explicitly?)

Together, the First and Second Stages set the scene for tackling more
pernicious sources of correlated noise in \EPRV.
These more pernicious sources are the subject of the Third Stage.

\section{Third Stage: Mitigation of myriad variabilities (stochastics)}

The Third Stage is to consider the incoherent variabilities of stars
that are not well understood theoretically, or which imprint
stochastic time-variable signatures on both the spectrum and the
derived \RV\ measurements.
These include (but are not limited to):
\begin{itemize}
\item
\textbf{star spots:}
Star spots create effective \RV\ perturbations because they reduce the
emission from a part of the rotating stellar surface.
That affects \EPRV\ to zeroth order, but at first order there are other
effects of star spots:
They are localized in velocity space, and they have a different
temperature from the surface of the star.
Therefore they ought to be visible in the two-dimensional space of
spectrum and wavelength as a cooler stellar patch slightly more
localized in velocity than the whole stellar surface.
Our approach on star spots, building on prior work (\citealt{Gully}) will
be to consider lower-temperature spectral components with small
velocity shifts try to fit everything simultaneously.
If the spots are reliably lower in temperature, it should be possible
to detect and mitigate them using physical photosphere models, at
least to first order.
This modeling is possible to implement as an extremely straightforward
extension of the \wobble\ framework.
\item
\textbf{plages, faculae, and other surface features:}
These features are similar to star spots and will imprint in similar
ways.
The main difference between star spots and other kinds of surface
features is that we don't have a good idea of the intrinsic spectral
properties of other kinds of features, which can show up as chemical
anomalies (at least in hot stars; \eg, \citealt{doppler}),
temperature anomalies (\eg\ \citealt{Gully}, \citealt{Milbourne2019}),
or mixtures of the two.
If we want to be flexible about surface features, we want to model
them as we propose for star spots, but permitting their spectral
properties to be determined by the data directly.
This is also possible as an extension of the \wobble\ framework.
Indeed it is very natural because in the current \wobble\ setup, the
star and tellurics are both modeled in a completely data-driven way.
In addition to all this, both spots and surface features should show
time coherence, as they rotate in and out of view on the stellar
surface.
As noted in the description of the First Stage (information), this
time coherence could be critical for identifying these effects and
separating them from the \RV\ signals from exoplanets.
\item
\textbf{flares and magnetic activity:}
The concept of ``stellar activity'' is often combining effects of
magnetic flares and star spots.
From our perspective, star spots appear in the data as rotating
surface patches, whereas flares appear as emission lines and possibly
also effective temperature changes in particular lines, again possibly
also with velocity shifts relative because of rotation or
chromospheric velocities.
Because we don't know exactly what to expect here, we might start by
regressing spectra or spectral residuals (away from \wobble\ fits)
against activity indicators to find full-spectrum indicators of
magnetic activity (work along these lines is also happening in the
Cisewski group at Yale; we know this from private communications).
If we can build a low-dimensional data-driven model (from, say, these
regressions or a latent-variable extension), we could hope to correct
the spectra for flaring, and measure the \RV\ off the corrected
spectrum.
\item
\textbf{surface convection:}
Hot material wells up and cold material sinks on the surface of the star.
On average these motions cancel, but they don't fully cancel spectrally.
Thus there are convective blueshifts in stellar spectra.
But the variance of this process also ought to have spectral
properties, and also some time coherence because the convection
pattern is a smoothly varying, slowly rotating property of the stellar
surface.
Our hope is that even this extremely stochastic process will have
low-dimensional structure in the spectral domain (it's mainly
temperature, after all, and some non-LTE effects) and also have
valuable covariance structure in the time domain.
If our hopes are justified, it will be possible to fit a stochastic
process (like a Gaussian Process) to the time domain, in a low-rank
representation of the spectral domain.
That's speculative, but it is worth finding out if it is true.
\end{itemize}

\noindent
We will not consider all of the possible sources of stochastic variability;
the project staffing and timeline are not large enough.
We will instead prioritize around the intersection of team interests (and
especially GRA interests), opportunity relative to other community efforts
(to use our resources for highest impact), and opportunity in terms of
impact on \EPRV\ projects (amplitudes of signals, flowed down to final measurements).
Since we don't yet understand these variabilities in detail, part of this
proposal is to do the work to make those priority decisions.

We emphasize here that we are \textit{not} proposing to ``solve
all stellar variability.'' What we are proposing to do is develop a set of 
principled, largely data-driven methods and statistical ideas, which may be 
broadly adapted to different \EPRV\ challenges. This Third Stage is the 
point at which we use the knowledge gained in the First Stage (where we 
laid out an information-theory-based framework) and in the Second Stage 
(where we tackled a relatively well-understood and tractable example 
of stellar spectral variability) and apply it to more general and perhaps
less well-behaved sources of noise.

As we said in the Introduction, the key idea is that the form of the
data-driven models we build match the form of the variability that
we expect.
So, for example, when we build a model to capture star spots, we want
to give the model the structure that we expect to have in star spots,
but the flexibility to adapt to the data.
This might look like adding to the \wobble\ model extra spectral
components that have the flexibility to look like cooler star surface
spots, and which have velocities relative to the mean stellar surface
velocity in the range permitted by the stellar rotation ($v\sin i$).
It might additionally involve requiring observations that are close
in time, to have these additional spot components have velocities that
are close in velocity space.
After all, we expect star spots to be patches of star with a different
spectrum, moving continuously relative to the stellar mean velocity.
Such additional components could then be optimized along with the other
components of \wobble.
That's just an example, but shows the kinds of approaches we imagine taking.

Again, for this Third Stage (stochastics), we will do experiments on
\EXPRES\ data we have in hand, and
public \HARPS\ data from the Archive.
That is, all the data necessary for this Stage is already in place.
In addition, we expect more relevant data to appear in public archives
from the \ESPRESSO\ and \NEID\ instruments.
We will adapt to new data as it becomes available.
For some of these projects we could benefit from Solar
spectra, since the Sun is a great example of the kind of star we are
intested in; by construction, it is Sun-like!
And data can be taken with very good signal-to-noise and time resoluion.
There are both \HARPS-N and \HARPS\ data on the Sun that will become
public during the project duration. 
\EXPRES\ and \NEID\ will also likely obtain Sun-as-a-star data. 
So many excellent sources of data exist for our use in the Third Stage.

\textbf{The deliverables of the Third Stage (stochastics) will be papers in the
refereed literature about stochastic sources of stellar spectral
variability.}
These publications will also include numerical methods, open-source software, 
documentation, and tutorials, as with previous works published by the 
proposing team \citep[e.g.][]{Bedell2019}.
Dissemination will be through papers, conference participation, and
\textbf{community workshops} we run in Years 2 and 3 (more detail below).

\section{Timeline and project management}

Although the three parts of this project are presented as ``stages'',
they will not be staged serially.
Rather we will run all three in parallel to make best use of our
personnel and benefit most from the conceptual and data overlaps of the three stages.
All three stages will involve PI Hogg, Co-I Bedell, and the project
graduate student student (\GRA). 

The First Stage (information) will be led by PI Hogg, with experiments (the
toy-model experiments) performed by the \GRA, and writing by the full
team.
It will occupy parts of Years~1 and 2, producing a paper
in each of those two years.
One paper will focus on the information-theoretic approach to the problem,
and the second will focus on the causal-inference approaches.

The Second Stage (p-modes) will be led by Co-I Bedell, with
experiments performed by the \GRA, and writing by the full team.
(HOGG: The preceding plan is no longer strictly true, 
but how to update without seeming like we're asking for money for 
something already done?)
The scientific analysis will take place in Years~1 and 2, with
writing and dissemination in Years~2 and 3.
This stage will produce one paper focused on direct comparison of 
passive approaches (nulling) and active approaches (resolving
and fitting) to treating the same data. 
The dissemination part will also include two community workshops on
\EPRV\ as a time-domain problem, in which we build on our experience
of working with radial-velocity teams to bring new methods and
software practices to the community.
These workshops will happen in Years~2 and 3.
They are important to the project because some of the approaches we
will be advocating will be ``outside the box'' for some projects.

The Third Stage (stochastics) will have different components led by PI
Hogg, Co-I Bedell, and the \GRA, with a schedule that depends on the
particular interests of the \GRA.
Each of the sources of stochastic variability will have different
signatures in time--spectrum space (as discussed above); we will use
Year~1 to do exploratory work in \wobble\ (\citealt{Bedell2019})
model residuals to set priorities
(which projects will be easier to see in the data, which harder).
That prioritization will create a schedule for projects, with data
analyses and experiments performed by the \GRA, and projects supervised
by PI Hogg and Co-I Bedell as it makes sense.
Some of the exploratory experiments will produce publishable results
to write up in Year~2 and more holistic mitigation will require
methods and software that will be built in Years~2 and 3 and published
and disseminated in Year~3.
Again, we will use the community workshops in Years~2 and 3 as part of
the dissemination.

PI Hogg will be responsible for overall project management. This is a
small-scale project; most of the management will involve decisions about our
priorities given the experiments we perform in Year~1 and the
challenges we discover there.
PI Hogg will also be responsible for academic supervision of, and
career mentoring for, the \GRA.
The two workshops will be chaired by Co-I Bedell, with PI Hogg and
the \GRA\ on the organizing committees.

HOGG SAY STUFF ABOUT BEDELL SUPPORT HERE. (and also SF facilities for hack weeks?)

\section{Prior NSF support}

Here we list \NSF\ grants that have supported the PI within the last five years.

\paragraph{\acronym{OAC-1841594}
\project{Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics} (2018-2020)}
This one-year (plus extension) grant (for \$36K) is part of a multi-institutional project
to design and propose a national center for multi-messenger astrophysics. The grant supported
meetings, travel, and writing by PI Hogg and Federica Bianco (now at University of Delaware).

\paragraph{\acronym{AST-1517237}
\project{New Probabilistic Methods for Observational Cosmology}
(2015--2019)} This grant (for \$328K) supported research into Bayesian
methods for cosmology. It mainly supported research by PI Hogg and
Alex Malz (now at University of Bochum). The grant proposal was co-written
by Dr Malz and established his career.
Under this grant, Dr Malz became one of the leaders in planning cosmological
observations with the \project{Large Synoptic Survey Telescope}.
This grant supported or partially supported 24 publications
in the refereed literature and one in preparation.

\paragraph{\acronym{IIS-1124794}
\project{A Unified Probabilistic Model of Astronomical Imaging} (2011--2016)}
This grant (for \$675K) was a Cyber-Enabled Discovery Type I Grant
which supported interdisciplinary research at the intersection of
astrophysics and computer vision.  It supported research by both
Hogg's group and the group of Prof Rob Fergus in Computer Science at
NYU.
The grant also supported research by multiple graduate students in the
Hogg and Fergus groups at NYU.
The grant supported both methodological and open-source software work
on \acronym{MCMC} methods, image processing, and spectral analysis.
It supported or partially supported
more than 40 refereed publications in the astronomical and
computer-vision literatures combined.
Work on this grant established the career of Ross Fadely, who built unusual
interdisciplinary expertise during the award period and who is now the
Chief of Data Science at the \textit{Wall St Journal}.

\paragraph{Others}
In addition to the recent grants listed above, the PI's group has been supported by
\acronym{AST-0908357}
\project{Dynamical models from kinematic data: The Milky Way Disk and Halo} (2009-2011),
and
\acronym{AST-0428465}
\project{Automated Astrometry for Time-Domain and Distributed Astrophysics} (2004--2007),
and
\acronym{PHY-0101738}
\project{Theoretical Particle Physics, Astrophysics and Cosmology} (2001--2004).
and (in ancient history) a \NSF\ graduate fellowship.
The majority of the PI's 220 refereed publications have been at least partially supported
by \NSF grants at a range of different institutions..

\section{Frequently asked questions}

Since this is a proposal, no-one has actually really asked us
questions \emph{frequently} about it.
But here's pretending!

\paragraph{Why do this now?}
There are a few extremely precise spectrographs operating now, 
and many more on the horizon \citep{Wright2017}.
\EPRV\ is entering a golden era, brought on in part by missions like
\TESS, which require solid follow-up, and in part by a hope that
\EPRV\ can detect the true Earth analogs (long-period, low-mass, rocky
planets around G-type stars).
If we don't solve these stellar spectral-variability issues, these
projects will not live up to their greatest promise.
And the three-year time scale of this grant is well matched to the
commissioning of many new instruments.

\paragraph{Isn't this proposal a giant fishing expedition?}
Kind of! It's true that we (and the community in general) are not yet certain 
which aspects of stellar variability are most critical to address, and which 
methods are best suited to mitigating them. We are not promising to 
``solve'' stellar activity or stellar spectral variability in this proposal.
Much of our work will be 
experimental. But this proposal is more than a series of experiments. 
We firmly believe that overcoming the hurdle of stellar variability and 
discovering Earth analogs will require data and effort from multiple, 
diverse \EPRV\ instruments and groups. To this end, we will establish 
a common language and statistically-motivated theoretical framework 
to guide the broader community. Through the papers produced in this 
program and through ``hack-week''-style events hosted by us, we aim to 
lay the groundwork that \textit{will} solve the problem of stellar variability 
in \EPRV, regardless of the success rate of the individual, granular experiments we do. 

\paragraph{How can these two influence the whole community?}
We might look like outsiders (and especially the PI).
But we are not: We have one of the very few open-source \EPRV\ codes
(\wobble; \citealt{Bedell2019}), and there are many new projects incorporating 
\wobble\ into their pipelines.
We are working on calibration and data analysis with the \EXPRES\ team (Debra Fischer
at Yale is the PI; see the letter of support).
Also, we are partners in the forthcoming \HARPS-3 project
\project{Terra Hunting Experiment}, which has a direct engineering
requirement to obtain better than $20\,\cmps$ precision
\RV\ measurements for quiet G-type stars.
\project{Terra Hunting} needs our help on stellar variability, and will adopt
the mitigation strategies we discover and build.
We have also operated many very successful workshops and hack weeks
bringing new methods and new data to the astrophysics community.
These include the \project{Gaia Sprints}, \project{Preparing for
  \TESS}, and the \project{Telluric-line Hack Week}.
These events have brought together diverse communities and spread
the gospel of good methods.
But above all, the whole \EPRV\ community is very thirsty for new
successes in mitigating noise. If we succeed in the Second and Third
Stages of this project, we will have no trouble developing community
interest.

\paragraph{Isn't this already being done?}
It certainly is true that the community is working hard on time
variability in some \EPRV\ contexts.
However, most of the work (with very notable exceptions) has centered
around stellar activity, and most of the work has centered on the
cross-correlation function.
It is our view that this is such a tall pole for \EPRV\ that we need
to launch and support multiple groups working from multiple angles.
We will be different from the community in a number of ways, but one
is that we are considering fully data-driven approaches, and in the
full outer-product space of time and wavelength.
We also have a conceptual bent: We will try to change the conversation
about \EPRV\ and then build mitigation strategies that talk to the
new frameworks and languages.
We have unique things to provide.
This does not represent a duplication of effort.

\paragraph{Isn't it all just about getting better instrument stability and calibration?}
No! New spectrographs are consistently getting few-$\cmps$ consistency
in internal calibration.
The problems are external---in the star and in the atmosphere.
Of course the telluric absorption and micro-tellurics are also
probably important, and we are working on those issues too (\citealt{Bedell2019}).
But there is no approach to $10\,\cmps$-level \EPRV\ without a direct
attack on stellar spectral variability.

\paragraph{You can't ever really get rid of stochastic stellar noise, right?}
If something is called ``noise'' we tend to think ``oh well, it's
noisy''. But if noise is structured---in spectral space or temporal
space or both---then it can be modeled continuously.
This proposal argues that all the important noise sources in \EPRV\ coming
from spectral variability will be structured in both time and wavelength.
Therefore there are handles for modeling and mitigation.
We are optimistic that these approaches will work.
If they don't, at least we will know that we have tried very very hard.

\paragraph{How does this connect to the CDS\&E program priorities?}
HOGG, BEDELL: TBD

\clearpage
\bibliographystyle{apalike}
\raggedright
\bibliography{description.bib}%general,myref,inprep}

\end{document}
